{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jgp5cd7fPk16",
        "outputId": "d5094994-3484-47ed-8645-d299549a9cd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in c:\\users\\bhati\\anaconda3\\lib\\site-packages (0.16.4)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from wandb) (8.0.4)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from wandb) (3.1.42)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\bhati\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (5.9.6)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from wandb) (1.41.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: setproctitle in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from wandb) (68.0.0)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from wandb) (4.23.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from Click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
            "Requirement already satisfied: six>=1.4.0 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Data...\n",
            " Dataset Loaded !!\n"
          ]
        }
      ],
      "source": [
        "#importing library as well loading datasets and spliting\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "print(\"Loading Data...\")\n",
        "from keras.datasets import fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1)\n",
        "print(\" Dataset Loaded !!\")\n",
        "\n",
        "\n",
        "\n",
        "# importing only libraries\n",
        "\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import math\n",
        "# import matplotlib.pyplot as plt\n",
        "# import wandb\n",
        "# from keras.datasets import fashion_mnist,mnist\n",
        "# from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 243,
      "metadata": {},
      "outputs": [],
      "source": [
        "# normalizing values and reshaping training values\n",
        "\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "x_val = x_val / 255\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "x_val = x_val.reshape(x_val.shape[0], -1)\n",
        "\n",
        "#Global Variables\n",
        "beta = 0.9\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "input_size = x_test.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-g_fAFBO_Sp",
        "outputId": "2bc2d663-ecf6-4fa3-b376-0cc6b77475fd"
      },
      "outputs": [],
      "source": [
        "def sigmoid( x):\n",
        "    x = np.clip(x,500,-500)\n",
        "    temp2 = np.exp(-x)\n",
        "    temp = 1.0/(1.0 + temp2)\n",
        "    return temp\n",
        "\n",
        "def sigmoid_derivative( x):\n",
        "    sigm = sigmoid(x)\n",
        "    temp = sigm\n",
        "    temp2 = 1 - sigm \n",
        "    return sigm*(1-sigm)\n",
        "\n",
        "def Confunsion_Matrix_Plot(y_pred, y):\n",
        "    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "    wandb.log({\"Confunsion_Matrix_Plot \": wandb.plot.confusion_matrix(probs = None, y_true = y, preds = y_pred, class_names = class_names)})\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    cm = confusion_matrix(y_pred, y)\n",
        "    print(cm)\n",
        "\n",
        "def relu( x):\n",
        "    return np.where(x > 0, x, 0)\n",
        "\n",
        "\n",
        "def relu_derivative( x):\n",
        "    return np.where(x < 0, 0, 1)\n",
        "\n",
        "\n",
        "\n",
        "def tanh( x):\n",
        "    t = np.tanh(x)\n",
        "    return t \n",
        "\n",
        "\n",
        "def tanh_derivative( x):\n",
        "    temp = np.tanh(x)\n",
        "    temp2 = 1 - temp*temp\n",
        "    return temp2\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "def softmax( x):\n",
        "    i = 0\n",
        "    while i < x.shape[0]:\n",
        "        argmax = np.argmax(x[i])\n",
        "        sum = 0\n",
        "        maxval = x[i][argmax]\n",
        "        j = 0\n",
        "        while j < x.shape[1]:\n",
        "            sum = sum + np.exp(x[i][j]-maxval)\n",
        "            j += 1\n",
        "        x[i] = np.exp(x[i]-maxval)/sum\n",
        "        i+=1\n",
        "    return x\n",
        "  \n",
        "def softmax_derivative( x):\n",
        "    temp = softmax(x)\n",
        "    temp2 = 1 - temp\n",
        "    return temp*temp2\n",
        "\n",
        "\n",
        "def one_hot_encoded( y, size):\n",
        "    temp = np.eye(size)[y]\n",
        "    return temp\n",
        "\n",
        "\n",
        "def cross_entropy( y_train, y_hat):\n",
        "    loss = 0\n",
        "    i = 0\n",
        "    while i < y_hat.shape[0]:\n",
        "        loss += -(np.log2(y_hat[i][y_train[i]]))\n",
        "        i += 1\n",
        "    return loss/y_hat.shape[0]\n",
        "\n",
        "def batch_converter(x1, y1, batch_size1):\n",
        "    x, y, batch_size = x1, y1, batch_size1\n",
        "    x_batch = []\n",
        "    y_batch = []\n",
        "    num_datapoints = x.shape[0]\n",
        "    no_datapoints = num_datapoints\n",
        "    no_batches = no_datapoints // batch_size                   # floor division operator\n",
        "    i = 0\n",
        "    while i < no_batches:\n",
        "        e = 0\n",
        "        if (i+1)*batch_size < x.shape[0]:\n",
        "            e = (i+1)*batch_size\n",
        "        else:\n",
        "            e = x.shape[0]\n",
        "        s = i*batch_size\n",
        "        x1 = np.array(x[s:e])        # slicing\n",
        "        y1 = np.array(y[s:e])        # slicing\n",
        "        x_batch.append(x1)\n",
        "        y_batch.append(y1)\n",
        "        i += 1\n",
        "    # jo datapoints last me bach jayenge wo yaha pe add kr rhe\n",
        "    temp = no_batches * batch_size\n",
        "    if temp != x_train.shape[0]:\n",
        "        x1 = np.array(x_train[temp :])\n",
        "        y1 = np.array(y_train[temp :])\n",
        "        x_batch.append(x1)\n",
        "        y_batch.append(y1)\n",
        "    return x_batch, y_batch\n",
        "\n",
        "def Confusion_Matrix_Plot(y_pred, y):\n",
        "    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "    \n",
        "    cm = confusion_matrix(y, y_pred)\n",
        "    \n",
        "    cm_percent = np.round(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100, decimals=2)\n",
        "    \n",
        "    \n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm_percent, annot=True, fmt=\".2f\", cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix (Percentage)')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Log the confusion matrix plot using Weights & Biases\n",
        "    wandb.log({\"Confusion_Matrix_Plot\": plt})\n",
        "\n",
        "def squared_error( y_train, y_hat, no_of_classes):\n",
        "    y_onehot = one_hot_encoded(y_train, no_of_classes)\n",
        "    loss = 0\n",
        "    i = 0\n",
        "    while i < y_hat.shape[0]:\n",
        "        loss += np.sum((y_hat[i] - y_onehot[i])**2)\n",
        "        i+=1\n",
        "    return loss / y_train.shape[0]\n",
        "\n",
        "class neural_network:\n",
        "  def __init__(self, s, weight_initialisation):\n",
        "    self.W, self.B, self.preactivation, self.activation = [],[],[],[]\n",
        "    self.initializer = weight_initialisation\n",
        "    self.network = s\n",
        "    self.initializeWandB()\n",
        "\n",
        "  def initializeWandB(self):\n",
        "    if self.initializer.lower() != \"random\":\n",
        "      i = 1\n",
        "      lengtht = len(self.network)\n",
        "      while i < lengtht:\n",
        "        temp = 6 / (self.network[i] + self.network[i-1])\n",
        "        n = np.sqrt(temp)\n",
        "        w = np.random.uniform(-n , n, (self.network[i], self.network[i-1]))\n",
        "        self.W.append(w)\n",
        "        b = np.random.uniform(-n , n, (self.network[i]))\n",
        "        self.B.append(b)\n",
        "        i += 1\n",
        "    # Random weight Initialisation\n",
        "    elif self.initializer.upper() != \"XAVIER\":\n",
        "      i = 1\n",
        "      while i < len(self.network):\n",
        "        b = np.random.randn(self.network[i])\n",
        "        self.B.append(b)\n",
        "        w = np.random.randn(self.network[i], self.network[i-1]) /(np.sqrt(self.network[i]))\n",
        "        self.W.append(w)\n",
        "        i += 1\n",
        "\n",
        "  # not normalizing\n",
        "\n",
        "  def loss_function(self, y_train, y_hat, no_of_classes, loss_func, lambd):\n",
        "    temp = y_train.shape[0]\n",
        "    loss = self.l2_regularize(lambd, temp)\n",
        "    if loss_func.upper() != \"CROSS_ENTROPY\":\n",
        "      loss = loss + squared_error(y_train, y_hat, no_of_classes)\n",
        "    else:\n",
        "      loss = loss + cross_entropy(y_train, y_hat)\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "  # hadmard gone\n",
        "\n",
        "\n",
        "  def l2_regularize(self, lambd, batch_size):\n",
        "    acc = 0\n",
        "    i = 0\n",
        "    while i < len(self.W):\n",
        "      acc += np.sum(self.W[i] ** 2)\n",
        "      i += 1\n",
        "    temp = (lambd/(2.* batch_size)) * acc\n",
        "    return temp\n",
        "\n",
        "  def forward(self, input, size1, activation_function1):\n",
        "    # Calculating for the hiddlen layers\n",
        "    size, activation_function = size1, activation_function1\n",
        "    i = 0\n",
        "    temp2 = activation_function.upper()\n",
        "    temp = len(size)-1\n",
        "    while i < temp:\n",
        "      Y = np.dot(input, self.W[i].T) + self.B[i]\n",
        "      \n",
        "      '''Not normalizing'''\n",
        "      if i < len(size)-2:\n",
        "        if i < len(self.preactivation):\n",
        "          self.preactivation[i] = Y\n",
        "        else:\n",
        "          self.preactivation.append(Y)\n",
        "        # Y_dash = self.normalize(Y)\n",
        "        if temp2 == \"RELU\":\n",
        "          Z = relu(Y)\n",
        "        if temp2 ==\"SIGMOID\":\n",
        "          Z = sigmoid(Y)\n",
        "        if temp2 ==\"TANH\":\n",
        "          Z = tanh(Y)\n",
        "        \n",
        "        temp3 = len(self.activation)\n",
        "        if i < temp3:\n",
        "          self.activation[i] = Z\n",
        "        else:\n",
        "          self.activation.append(Z)\n",
        "        input = Z\n",
        "      else:\n",
        "        #Calculating for the output layer.\n",
        "        Y = np.dot(input, self.W[i].T) + self.B[i]\n",
        "        # Y = self.normalize(Y)\n",
        "        temp4 =len(self.preactivation)\n",
        "        if i < temp4:\n",
        "          self.preactivation[i] = Y\n",
        "        else:\n",
        "          self.preactivation.append(Y)\n",
        "        Z = softmax(Y)\n",
        "        temp5 = len(self.activation)\n",
        "        if i < temp5:\n",
        "          self.activation[i] = Z\n",
        "        else:\n",
        "          self.activation.append(Z)\n",
        "      i = i + 1\n",
        "    return self.preactivation, self.activation\n",
        "\n",
        "\n",
        "  def backward(self, layers1, x1, y1 ,no_of_classes1, preac1, ac1, activation_function1, loss_func1):\n",
        "    layers, x, y ,no_of_classes, preac, ac, activation_function, loss_func = layers1, x1, y1 ,no_of_classes1, preac1, ac1, activation_function1, loss_func1\n",
        "    no_layers = len(layers)\n",
        "    grad_a, grad_w, grad_b, grad_h = [],[],[],[]\n",
        "    y_onehot = one_hot_encoded(y, no_of_classes)\n",
        "    temp4 = activation_function.upper()\n",
        "    temp = \"cross_entropy\"\n",
        "    temp2 = len(ac)-1\n",
        "    if loss_func.lower() == temp:\n",
        "\n",
        "      temp = -(y_onehot - ac[temp2])\n",
        "      grad_a.append(temp)\n",
        "    else: #MSE\n",
        "      grad_a.append((ac[temp2] - y_onehot) * softmax_derivative(ac[temp2]))#ac[len(ac)-1]*(1 - ac[len(ac)-1]))\n",
        "    i = no_layers - 2\n",
        "    while i  > -1:\n",
        "      temp3 = no_layers-2-i\n",
        "      if i == 0:\n",
        "        dw = (grad_a[temp3].T @ x) #/ y.shape[0]\n",
        "        db = np.sum(grad_a[temp3],axis=0)/y.shape[0]\n",
        "      elif i > 0:\n",
        "        dw = (grad_a[temp3].T @ ac[i-1])#/ y.shape[0]\n",
        "        db = np.sum(grad_a[temp3],axis=0)/ y.shape[0]\n",
        "        dh_1 = grad_a[temp3] @ self.W[i]\n",
        "        sig = 0\n",
        "        if temp4 == \"SIGMOID\":\n",
        "          sig = sigmoid_derivative(preac[i-1])\n",
        "        if temp4 == \"RELU\":\n",
        "          sig = relu_derivative(preac[i-1])\n",
        "        if temp4 == \"TANH\":\n",
        "          sig = tanh_derivative(preac[i-1])\n",
        "        \n",
        "\n",
        "        da_1 = dh_1 * sig\n",
        "\n",
        "        grad_h.append(dh_1)\n",
        "        grad_a.append(da_1)\n",
        "      grad_b.append(db)\n",
        "      grad_w.append(dw)\n",
        "      i -= 1\n",
        "    return grad_w, grad_b\n",
        "\n",
        "\n",
        "  def printingFunction(self, loss_train, loss_val, accur_train, accur_val, i, optimizer_name):\n",
        "      \n",
        "      print(i+1, \"Iteration No : \", \"\\t Train Loss\\t\", loss_train)\n",
        "      print(i+1, \"Iteration No : \", \"\\t Validate Loss\\t\", loss_val)\n",
        "      print(i+1, \"Iteration No : \", \"\\t Train Accuracy\\t\", accur_train)\n",
        "      print(i+1, \"Iteration No : \", \"\\t Validate Accuracy\\t\", accur_val)\n",
        "      print(\"---------------------------------------------------------\")\n",
        "\n",
        "  def batch_grad_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, activation_function, eta, batch_size, n_iterations, loss_func, lambd, do_wandb_log):\n",
        "    x_batch, y_batch = batch_converter(x_train, y_train, batch_size)\n",
        "    loss_arr = []\n",
        "    i = 0\n",
        "    length = len(layers)\n",
        "    while i < n_iterations:\n",
        "      j = 0\n",
        "      while j < len(x_batch):\n",
        "        xb, yb = x_batch[j], y_batch[j]\n",
        "        preac, ac = None, None\n",
        "        def f(xb, layers, activation_function, yb, no_of_classes, preac, ac, loss_func):\n",
        "          preac, ac = self.forward(xb, layers, activation_function)\n",
        "          grad_w, grad_b = self.backward(layers, xb, yb, no_of_classes,preac, ac, activation_function, loss_func)\n",
        "          return preac, ac, grad_w, grad_b\n",
        "        preac, ac, grad_w, grad_b = f(xb, layers, activation_function, yb, no_of_classes, preac, ac, loss_func)\n",
        "        l = 0\n",
        "        while l < length-1:\n",
        "          # print(\"shape\",self.W[l].shape, grad_w[length-l-2].shape)\n",
        "          self.W[l] += -(eta * grad_w[length-l-2] + eta * lambd * self.W[l])\n",
        "          self.B[l] += -eta * grad_b[length-l-2]\n",
        "          l += 1\n",
        "        j += 1\n",
        "\n",
        "      loss_train = 0\n",
        "      loss_val = 0\n",
        "\n",
        "      def f2(x_train, layers, activation_function, y_train, ac, no_of_classes, loss_func, lambd, x_test, y_test):\n",
        "        preac, ac = self.forward(x_train, layers, activation_function)\n",
        "        loss_train = self.loss_function(y_train, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "\n",
        "        preac, ac = self.forward(x_test, layers, activation_function )\n",
        "        loss_val = self.loss_function(y_test, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "        return loss_train, loss_val\n",
        "\n",
        "      loss_train, loss_val = f2(x_train, layers, activation_function, y_train, ac, no_of_classes, loss_func, lambd, x_test, y_test)\n",
        "      loss_arr.append(loss_val)\n",
        "      accur_train = self.test_accuracy(layers, x_train, y_train, activation_function)\n",
        "      accur_val = self.test_accuracy(layers, x_test, y_test, activation_function)\n",
        "\n",
        "      name = \"SGD\"\n",
        "      self.printingFunction(loss_train, loss_val, accur_train, accur_val, i, name)\n",
        "\n",
        "\n",
        "      # print(i+1, \"Iteration No : \\t\\t\", \"\\t Train Loss\\t\\t\", loss_train)\n",
        "      # print(i+1, \"Iteration No : \\t\\t\", \"\\t Validate Loss\\t\\t\\n\", loss_val)\n",
        "      # print(i+1, \"Iteration No : \\t\\t\", \"\\t Train Accuracy\\t\\t\", accur_train)\n",
        "      # print(i+1, \"Iteration No : \\t\\t\", \"\\t Validate Accuracy\\t\\t\", accur_val)\n",
        "      # print(\"---------------------------------------------------------\")\n",
        "      if do_wandb_log == True:\n",
        "        wandb_plots = dict({\"epoch\": i+1 , \"train_accuracy\":accur_train,\"train_error\":loss_train,\"val_accuracy\":accur_val,\"val_error\":loss_val})\n",
        "        wandb.log(wandb_plots)\n",
        "      i += 1\n",
        "    return ac[len(ac)-1], y_test\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "  def momentum_grad_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, activation_function, batch_size, eta, epochs, beta, loss_func,lambd, do_wandb_log):\n",
        "    l = len(layers)\n",
        "    prev_w, prev_b, loss_arr = [],[],[]\n",
        "    i = 0\n",
        "    while i < l-1:\n",
        "      prev_w.append(np.zeros(self.W[i].shape))\n",
        "      prev_b.append(np.zeros(self.B[i].shape))\n",
        "      i += 1\n",
        "    x_batch, y_batch = batch_converter(x_train, y_train, batch_size)\n",
        "    ep = 0\n",
        "    while ep < epochs:\n",
        "      j = 0\n",
        "      while j < len(x_batch):\n",
        "        xb = x_batch[j]\n",
        "        yb = y_batch[j]\n",
        "        preac, ac = self.forward(xb, layers, activation_function)\n",
        "        grad_w, grad_b = self.backward(layers, xb, yb, no_of_classes, preac, ac, activation_function, loss_func)\n",
        "        i = 0\n",
        "        while i < l-1:\n",
        "          prev_w[i] = beta*prev_w[i] + grad_w[l-i-2]\n",
        "          prev_b[i] = beta*prev_b[i] + grad_b[l-i-2]\n",
        "          self.W[i] += -(eta*prev_w[i] + eta * lambd * self.W[i])\n",
        "          self.B[i] += -eta*prev_b[i]\n",
        "          i += 1\n",
        "        j += 1\n",
        "\n",
        "      loss_train = 0\n",
        "      loss_val = 0\n",
        "\n",
        "      def f3(x_train, layers, activation_function, y_train, ac, no_of_classes, loss_func, lambd, x_test, y_test):\n",
        "\n",
        "        preac, ac = self.forward(x_train, layers, activation_function)\n",
        "        loss_train = self.loss_function(y_train, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "\n",
        "        preac, ac = self.forward(x_test, layers, activation_function)\n",
        "        loss_val = self.loss_function(y_test, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "        return loss_train, loss_val\n",
        "\n",
        "      loss_train, loss_val = f3(x_train, layers, activation_function, y_train, ac, no_of_classes, loss_func, lambd, x_test, y_test)\n",
        "      loss_arr.append(loss_val)\n",
        "      accur_train = self.test_accuracy(layers, x_train, y_train, activation_function)\n",
        "      accur_val = self.test_accuracy(layers, x_test, y_test, activation_function)\n",
        "\n",
        "\n",
        "\n",
        "      name = \"MOMENTUM\"\n",
        "      self.printingFunction(loss_train, loss_val, accur_train, accur_val, ep, name)\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "      if do_wandb_log == True:\n",
        "        wandb_plots = dict({\"epoch\": ep+1 , \"train_accuracy\":accur_train,\"train_error\":loss_train,\"val_accuracy\":accur_val,\"val_error\":loss_val})\n",
        "        wandb.log(wandb_plots)\n",
        "      ep += 1\n",
        "    return ac[len(ac)-1], y_test\n",
        "\n",
        "\n",
        "\n",
        "  def nesterov_gradient_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, activation_function, batch_size, eta, epochs, beta, loss_func,lambd, do_wandb_log):\n",
        "    l = len(layers)\n",
        "    prev_w, prev_b, loss_arr = [],[],[]\n",
        "    i = 0\n",
        "    while i < l-1:\n",
        "      prev_w.append(np.zeros(self.W[i].shape))\n",
        "      prev_b.append(np.zeros(self.B[i].shape))\n",
        "      i += 1\n",
        "    x_batch, y_batch = batch_converter(x_train, y_train, batch_size)\n",
        "\n",
        "    ep = 0\n",
        "    while ep < epochs:\n",
        "      j = 0\n",
        "      while j < len(x_batch):\n",
        "        xb = x_batch[j]\n",
        "        yb = y_batch[j]\n",
        "        i = 0\n",
        "        while i < l-1:\n",
        "          self.W[i] += -beta * prev_w[i]\n",
        "          self.B[i] += -beta * prev_b[i]\n",
        "          i += 1\n",
        "        preac, ac = self.forward(xb, layers, activation_function)\n",
        "        grad_w, grad_b = self.backward(layers, xb, yb, no_of_classes, preac, ac, activation_function, loss_func)\n",
        "        # print(\"grad_w\", grad_w)\n",
        "        i = 0\n",
        "        while i < l-1:\n",
        "          prev_w[i] = beta * prev_w[i] + grad_w[l-i-2]\n",
        "          prev_b[i] = beta * prev_b[i] + grad_b[l-i-2]\n",
        "          self.W[i] += -(eta * prev_w[i] + eta * lambd * self.W[i])\n",
        "          self.B[i] += -eta * prev_b[i]\n",
        "          i += 1\n",
        "        j += 1\n",
        "\n",
        "\n",
        "      loss_train = 0\n",
        "      loss_val = 0\n",
        "\n",
        "      def f4(x_train, layers, activation_function, y_train, ac, no_of_classes, loss_func, lambd, x_test, y_test):\n",
        "\n",
        "        preac, ac = self.forward(x_train, layers, activation_function)\n",
        "        loss_train = self.loss_function(y_train, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "\n",
        "        preac, ac = self.forward(x_test, layers, activation_function)\n",
        "        loss_val = self.loss_function(y_test, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "        return loss_train, loss_val\n",
        "\n",
        "      loss_train, loss_val = f4(x_train, layers, activation_function, y_train, ac, no_of_classes, loss_func, lambd, x_test, y_test)\n",
        "      loss_arr.append(loss_val)\n",
        "      accur_train = self.test_accuracy(layers, x_train, y_train, activation_function)\n",
        "      accur_val = self.test_accuracy(layers, x_test, y_test, activation_function)\n",
        "\n",
        "\n",
        "\n",
        "      name = \"NAG\"\n",
        "      self.printingFunction(loss_train, loss_val, accur_train, accur_val, ep, name)\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "      if do_wandb_log == True:\n",
        "        wandb_plots = dict({\"epoch\": ep+1 , \"train_accuracy\":accur_train,\"train_error\":loss_train,\"val_accuracy\":accur_val,\"val_error\":loss_val})\n",
        "        wandb.log(wandb_plots)\n",
        "      ep += 1\n",
        "    return ac[len(ac)-1], y_test\n",
        "\n",
        "  def nesterov_gradient_descent_(self, x_train, y_train, x_test, y_test, no_of_classes, layers, activation_function, batch_size, eta, epochs, beta, loss_func,lambd, do_wandb_log):\n",
        "    eps = 1e-4\n",
        "    vw, vb, loss_arr = [],[],[]\n",
        "    l = len(layers)\n",
        "    i = 0\n",
        "    while i < l-1:\n",
        "      vw.append(np.zeros(self.W[i].shape))\n",
        "      vb.append(np.zeros(self.B[i].shape))\n",
        "      i += 1\n",
        "\n",
        "    x_batch, y_batch = batch_converter(x_train, y_train, batch_size)\n",
        "    ep = 0\n",
        "    while ep < epochs:\n",
        "      j = 0\n",
        "      while j < len(x_batch):\n",
        "        xb = x_batch[j]\n",
        "        yb = y_batch[j]\n",
        "        preac, ac = self.forward(xb, layers, activation_function)\n",
        "        grad_w, grad_b = self.backward(layers, xb, yb, no_of_classes, preac, ac, activation_function, loss_func)\n",
        "        i = 0\n",
        "        while i < l-2:\n",
        "          vw[i] = beta * vw[i] + (1-beta) * grad_w[l-i-2] * grad_w[l-i-2]\n",
        "          vb[i] = beta * vb[i] + (1-beta) * grad_b[l-i-2] * grad_b[l-i-2]\n",
        "          self.W[i] =  self.W[i] - (eta * grad_w[l-i-2])/np.sqrt(vw[i] + eps) - (eta * lambd * self.W[i])\n",
        "          self.B[i] =  self.B[i] - (eta * grad_b[l-i-2])/np.sqrt(vb[i] + eps)\n",
        "          i += 1\n",
        "        vw[i] = beta * vw[i] + (1-beta) * grad_w[l-i-2] * grad_w[l-i-2]\n",
        "        vb[i] = beta * vb[i] + (1-beta) * grad_b[l-i-2] * grad_b[l-i-2]\n",
        "        self.W[i] =  self.W[i] - (eta * grad_w[l-i-2])/np.sqrt(vw[i] + eps) - (eta * lambd * self.W[i])\n",
        "        self.B[i] =  self.B[i] - (eta * grad_b[l-i-2])/np.sqrt(vb[i] + eps)\n",
        "        j += 1\n",
        "\n",
        "\n",
        "      loss_train = 0\n",
        "      loss_val = 0\n",
        "\n",
        "      def f5(x_train, layers, activation_function, y_train, ac, no_of_classes, loss_func, lambd, x_test, y_test):\n",
        "\n",
        "        preac, ac = self.forward(x_train, layers, activation_function)\n",
        "        loss_train = self.loss_function(y_train, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "\n",
        "        preac, ac = self.forward(x_test, layers, activation_function)\n",
        "        loss_val = self.loss_function(y_test, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "        return loss_train, loss_val\n",
        "\n",
        "      loss_train, loss_val = f5(x_train, layers, activation_function, y_train, ac, no_of_classes, loss_func, lambd, x_test, y_test)\n",
        "      loss_arr.append(loss_val)\n",
        "      accur_train = self.test_accuracy(layers, x_train, y_train, activation_function)\n",
        "      accur_val = self.test_accuracy(layers, x_test, y_test, activation_function)\n",
        "\n",
        "      \n",
        "      name = \"NAG_\"\n",
        "      self.printingFunction(loss_train, loss_val, accur_train, accur_val, ep, name)\n",
        "\n",
        "\n",
        "      if do_wandb_log == True:\n",
        "        wandb_plots = dict({\"epoch\": ep+1 , \"train_accuracy\":accur_train,\"train_error\":loss_train,\"val_accuracy\":accur_val,\"val_error\":loss_val})\n",
        "        wandb.log(wandb_plots)\n",
        "      ep += 1\n",
        "    return ac[len(ac)-1], y_test\n",
        "  def rmsprop_gradient_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, activation_function, batch_size, eta, epochs, beta, loss_func,lambd, do_wandb_log):\n",
        "    eps = 1e-4\n",
        "    vw, vb, loss_arr = [],[],[]\n",
        "    l = len(layers)\n",
        "    i = 0\n",
        "    while i < l-1:\n",
        "      vw.append(np.zeros(self.W[i].shape))\n",
        "      vb.append(np.zeros(self.B[i].shape))\n",
        "      i += 1\n",
        "\n",
        "    x_batch, y_batch = batch_converter(x_train, y_train, batch_size)\n",
        "    ep = 0\n",
        "    while ep < epochs:\n",
        "      j = 0\n",
        "      while j < len(x_batch):\n",
        "        xb = x_batch[j]\n",
        "        yb = y_batch[j]\n",
        "        preac, ac = self.forward(xb, layers, activation_function)\n",
        "        grad_w, grad_b = self.backward(layers, xb, yb, no_of_classes, preac, ac, activation_function, loss_func)\n",
        "        i = 0\n",
        "        while i < l-2:\n",
        "          vw[i] = beta * vw[i] + (1-beta) * grad_w[l-i-2] * grad_w[l-i-2]\n",
        "          vb[i] = beta * vb[i] + (1-beta) * grad_b[l-i-2] * grad_b[l-i-2]\n",
        "          self.W[i] =  self.W[i] - (eta * grad_w[l-i-2])/np.sqrt(vw[i] + eps) - (eta * lambd * self.W[i])\n",
        "          self.B[i] =  self.B[i] - (eta * grad_b[l-i-2])/np.sqrt(vb[i] + eps)\n",
        "          i += 1\n",
        "        vw[i] = beta * vw[i] + (1-beta) * grad_w[l-i-2] * grad_w[l-i-2]\n",
        "        vb[i] = beta * vb[i] + (1-beta) * grad_b[l-i-2] * grad_b[l-i-2]\n",
        "        self.W[i] =  self.W[i] - (eta * grad_w[l-i-2])/np.sqrt(vw[i] + eps) - (eta * lambd * self.W[i])\n",
        "        self.B[i] =  self.B[i] - (eta * grad_b[l-i-2])/np.sqrt(vb[i] + eps)\n",
        "        j += 1\n",
        "\n",
        "\n",
        "      loss_train = 0\n",
        "      loss_val = 0\n",
        "\n",
        "      def f5(x_train, layers, activation_function, y_train, ac, no_of_classes, loss_func, lambd, x_test, y_test):\n",
        "\n",
        "        preac, ac = self.forward(x_train, layers, activation_function)\n",
        "        loss_train = self.loss_function(y_train, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "\n",
        "        preac, ac = self.forward(x_test, layers, activation_function)\n",
        "        loss_val = self.loss_function(y_test, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "        return loss_train, loss_val\n",
        "\n",
        "      loss_train, loss_val = f5(x_train, layers, activation_function, y_train, ac, no_of_classes, loss_func, lambd, x_test, y_test)\n",
        "      loss_arr.append(loss_val)\n",
        "      accur_train = self.test_accuracy(layers, x_train, y_train, activation_function)\n",
        "      accur_val = self.test_accuracy(layers, x_test, y_test, activation_function)\n",
        "\n",
        "      \n",
        "      name = \"RMSPROP\"\n",
        "      self.printingFunction(loss_train, loss_val, accur_train, accur_val, ep, name)\n",
        "\n",
        "\n",
        "      if do_wandb_log == True:\n",
        "        wandb_plots = dict({\"epoch\": ep+1 , \"train_accuracy\":accur_train,\"train_error\":loss_train,\"val_accuracy\":accur_val,\"val_error\":loss_val})\n",
        "        wandb.log(wandb_plots)\n",
        "      ep += 1\n",
        "    return ac[len(ac)-1], y_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def adam_gradient_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, activation_function, batch_size, eta, epochs, beta1, beta2, loss_func,lambd, do_wandb_log):\n",
        "    l = len(layers)\n",
        "    mw, mb, vw, vb = [],[],[],[]\n",
        "    eps = 1e-10\n",
        "    loss_arr = []\n",
        "    i = 0\n",
        "    while i < l-1:\n",
        "      vb.append(np.zeros(self.B[i].shape))\n",
        "      mw.append(np.zeros(self.W[i].shape))\n",
        "      vw.append(np.zeros(self.W[i].shape))\n",
        "      mb.append(np.zeros(self.B[i].shape))\n",
        "      i += 1\n",
        "    x_batch, y_batch = batch_converter(x_train, y_train, batch_size)\n",
        "    ep = 0\n",
        "    while ep < epochs:\n",
        "      j = 0\n",
        "      while j < len(x_batch):\n",
        "        xb = x_batch[j]\n",
        "        yb = y_batch[j]\n",
        "        preac, ac = self.forward(xb, layers, activation_function)\n",
        "        grad_w, grad_b = self.backward(layers, xb, yb, no_of_classes, preac, ac, activation_function, loss_func)\n",
        "        i = 0\n",
        "        while i < l-1:\n",
        "          temp = l-i-2\n",
        "          mw[i] = beta1 * mw[i] + (1-beta1) * grad_w[temp]\n",
        "          mb[i] = beta1 * mb[i] + (1-beta1) * grad_b[temp]\n",
        "          mw_hat = mw[i] / (1 - np.power(beta1, j+1))\n",
        "          mb_hat = mb[i] / (1 - np.power(beta1, j+1))\n",
        "\n",
        "          vw[i] = beta2 * vw[i] + (1-beta2) * grad_w[temp] * grad_w[temp]\n",
        "          vb[i] = beta2 * vb[i] + (1-beta2) * grad_b[temp] * grad_b[temp]\n",
        "          vw_hat = vw[i] / (1 - np.power(beta2, j+1))\n",
        "          vb_hat = vb[i] / (1 - np.power(beta2, j+1))\n",
        "\n",
        "          self.W[i] = self.W[i] - (eta * mw_hat)/(np.sqrt(vw_hat) + eps)- (eta * lambd * self.W[i])\n",
        "          self.B[i] = self.B[i] - (eta * mb_hat)/(np.sqrt(vb_hat) + eps)\n",
        "          i += 1\n",
        "        j += 1\n",
        "      preac, ac = self.forward(x_train, layers, activation_function)\n",
        "      loss_train = self.loss_function(y_train, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "\n",
        "      preac, ac = self.forward(x_test, layers, activation_function)\n",
        "      loss_val = self.loss_function(y_test, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "\n",
        "      loss_arr.append(loss_val)\n",
        "      accur_train = self.test_accuracy(layers, x_train, y_train, activation_function)\n",
        "      accur_val = self.test_accuracy(layers, x_test, y_test, activation_function)\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "      name = \"ADAM\"\n",
        "      self.printingFunction(loss_train, loss_val, accur_train, accur_val, ep, name)\n",
        "\n",
        "\n",
        "      if do_wandb_log == True:\n",
        "        wandb_plots = dict({\"epoch\": ep+1 , \"train_accuracy\":accur_train,\"train_error\":loss_train,\"val_accuracy\":accur_val,\"val_error\":loss_val})\n",
        "        wandb.log(wandb_plots)\n",
        "      ep += 1\n",
        "    return ac[len(ac)-1], y_test\n",
        "\n",
        "\n",
        "  def nadam_gradient_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, activation_function, batch_size, eta, epochs, beta1, beta2, loss_func,lambd, do_wandb_log):\n",
        "    eps = 1e-10\n",
        "    mw, mb, vw, vb = [],[],[],[]\n",
        "    x_batch, y_batch = batch_converter(x_train, y_train, batch_size)\n",
        "    l = len(layers)\n",
        "    loss_arr = []\n",
        "    i = 0\n",
        "    while i < l-2:\n",
        "      vw.append(np.zeros(self.W[i].shape))\n",
        "      vb.append(np.zeros(self.B[i].shape))\n",
        "      mw.append(np.zeros(self.W[i].shape))\n",
        "      mb.append(np.zeros(self.B[i].shape))\n",
        "      i += 1\n",
        "    vw.append(np.zeros(self.W[i].shape))\n",
        "    vb.append(np.zeros(self.B[i].shape))\n",
        "    mw.append(np.zeros(self.W[i].shape))\n",
        "    mb.append(np.zeros(self.B[i].shape))\n",
        "    x_batch, y_batch = batch_converter(x_train, y_train, batch_size)\n",
        "    ep = 0\n",
        "    while ep < epochs:\n",
        "      j = 0\n",
        "      while j < len(x_batch):\n",
        "        xb = x_batch[j]\n",
        "        yb = y_batch[j]\n",
        "        preac, ac = self.forward(xb, layers, activation_function)\n",
        "        grad_w, grad_b = self.backward(layers, xb, yb, no_of_classes, preac, ac, activation_function, loss_func)\n",
        "        i = 0\n",
        "        while i < l-2:\n",
        "          mw[i] = beta1 * mw[i] + (1-beta1)* grad_w[l-i-2]\n",
        "          mb[i] = beta1 * mb[i] + (1-beta1)* grad_b[l-i-2]\n",
        "          mw_hat = mw[i] / (1 - np.power(beta1, j+1))\n",
        "          mb_hat = mb[i] / (1 - np.power(beta1, j+1))\n",
        "\n",
        "          vw[i] = beta2 * vw[i] + (1-beta2) * grad_w[l-i-2] * grad_w[l-i-2]\n",
        "          vb[i] = beta2 * vb[i] + (1-beta2) * grad_b[l-i-2] * grad_b[l-i-2]\n",
        "          vw_hat = vw[i] / (1 - np.power(beta2, j+1))\n",
        "          vb_hat = vb[i] / (1 - np.power(beta2, j+1))\n",
        "\n",
        "          self.W[i] = self.W[i] - (eta/(np.sqrt(vw[i]) + eps)) * (beta1 * mw_hat + (((1-beta1) * grad_w[l-i-2]) / (1 - np.power(beta1, j+1)))) - (eta * lambd * self.W[i])\n",
        "          self.B[i] = self.B[i] - (eta/(np.sqrt(vb[i]) + eps)) * (beta1 * mb_hat + (((1-beta1) * grad_b[l-i-2]) / (1 - np.power(beta1, j+1))))\n",
        "          i += 1\n",
        "        mw[i] = beta1 * mw[i] + (1-beta1)* grad_w[l-i-2]\n",
        "        mb[i] = beta1 * mb[i] + (1-beta1)* grad_b[l-i-2]\n",
        "        mw_hat = mw[i] / (1 - np.power(beta1, j+1))\n",
        "        mb_hat = mb[i] / (1 - np.power(beta1, j+1))\n",
        "\n",
        "        vw[i] = beta2 * vw[i] + (1-beta2) * grad_w[l-i-2] * grad_w[l-i-2]\n",
        "        vb[i] = beta2 * vb[i] + (1-beta2) * grad_b[l-i-2] * grad_b[l-i-2]\n",
        "        vw_hat = vw[i] / (1 - np.power(beta2, j+1))\n",
        "        vb_hat = vb[i] / (1 - np.power(beta2, j+1))\n",
        "\n",
        "        self.W[i] = self.W[i] - (eta/(np.sqrt(vw[i]) + eps)) * (beta1 * mw_hat + (((1-beta1) * grad_w[l-i-2]) / (1 - np.power(beta1, j+1)))) - (eta * lambd * self.W[i])\n",
        "        self.B[i] = self.B[i] - (eta/(np.sqrt(vb[i]) + eps)) * (beta1 * mb_hat + (((1-beta1) * grad_b[l-i-2]) / (1 - np.power(beta1, j+1))))\n",
        "        j += 1\n",
        "\n",
        "      loss_train = 0\n",
        "      loss_val = 0\n",
        "\n",
        "      def f5(x_train, layers, activation_function, y_train, ac, no_of_classes, loss_func, lambd, x_test, y_test):\n",
        "\n",
        "        preac, ac = self.forward(x_train, layers, activation_function)\n",
        "        loss_train = self.loss_function(y_train, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "\n",
        "        preac, ac = self.forward(x_test, layers, activation_function)\n",
        "        loss_val = self.loss_function(y_test, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "        return loss_train, loss_val\n",
        "\n",
        "      loss_train, loss_val = f5(x_train, layers, activation_function, y_train, ac, no_of_classes, loss_func, lambd, x_test, y_test)\n",
        "      loss_arr.append(loss_val)\n",
        "      accur_train = self.test_accuracy(layers, x_train, y_train, activation_function)\n",
        "      accur_val = self.test_accuracy(layers, x_test, y_test, activation_function)\n",
        "\n",
        "      # print(ep+1, \"Iteration No : \\t\\t\", \"\\t Train Loss\\t\\t\", loss_train)\n",
        "      # print(ep+1, \"Iteration No : \\t\\t\", \"\\t Validate Loss\\t\\t\", loss_val, \"\\n\")\n",
        "\n",
        "\n",
        "      # print(ep+1, \"Iteration No : \\t\\t\", \"\\t Train Accuracy\\t\\t\", accur_train)\n",
        "      # print(ep+1,\"Iteration No : \\t\\t\", \"\\t Validate Accuracy\\t\\t\", accur_val)\n",
        "      # print(\"---------------------------------------------------------\")\n",
        "      name = \"NADAM\"\n",
        "      self.printingFunction(loss_train, loss_val, accur_train, accur_val, ep, name)\n",
        "\n",
        "      if do_wandb_log == True:\n",
        "        wandb_plots = dict({\"epoch\": ep+1 , \"train_accuracy\":accur_train,\"train_error\":loss_train,\"val_accuracy\":accur_val,\"val_error\":loss_val})\n",
        "        wandb.log(wandb_plots)\n",
        "      ep += 1\n",
        "    return ac[len(ac)-1], y_test\n",
        "\n",
        "\n",
        "  def test_accuracy(self, layers1, x1, y1, activation_function1):\n",
        "    layers, x, y, activation_function = layers1, x1, y1, activation_function1\n",
        "    preac, ac = self.forward(x, layers, activation_function)\n",
        "    y_pred = ac[len(ac)-1]\n",
        "    err_count = 0\n",
        "    i = 0\n",
        "    while i < y_pred.shape[0]:\n",
        "      maxval = -(math.inf)\n",
        "      maxind = -1\n",
        "      j = 0\n",
        "      while j < y_pred.shape[1]:\n",
        "        if maxval < y_pred[i][j]:\n",
        "          maxval = y_pred[i][j]\n",
        "          maxind = j\n",
        "        j += 1\n",
        "      if maxind != y[i]:\n",
        "        err_count = err_count + 1\n",
        "      i += 1\n",
        "    temp = ((y.shape[0] - err_count)/y.shape[0])*100\n",
        "    return temp\n",
        "    \n",
        "  def PlotError(self, ErrorSum):\n",
        "    Iter = []\n",
        "    for i in range(len(ErrorSum)):\n",
        "      Iter.append(i)\n",
        "    plt.plot(Iter,ErrorSum)\n",
        "    plt.title('Error v/s Iteration')\n",
        "    plt.xlabel('No of Iterations')\n",
        "    plt.ylabel('Error')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "id": "Cuhbg19NPiKD"
      },
      "outputs": [],
      "source": [
        "def main(x_train1, y_train1, x_val1, y_val1, input_size1, no_hidden_layers1, hidden_layer_size1, no_of_classes1, wt_initialisation1, optimiser1, activation_function1, batch_size1, eta1, epoch1, momentum1, beta1, beta11, beta21, loss_func1, lambd1, do_wandb_log1, plot_conf_mat1):\n",
        "    com = optimiser1.upper() \n",
        "    layers = []\n",
        "    layers.append(input_size1)\n",
        "    i = 0\n",
        "    while i < no_hidden_layers1:\n",
        "      layers.append(hidden_layer_size1)\n",
        "      i+=1\n",
        "    layers.append(no_of_classes1)\n",
        "\n",
        "    x_train, y_train, x_val, y_val, input_size, no_hidden_layers, hidden_layer_size, no_of_classes, wt_initialisation, optimiser, activation_function, batch_size, eta, epoch, momentum, beta, beta1, beta2, loss_func, lambd, do_wandb_log, plot_conf_mat = x_train1, y_train1, x_val1, y_val1, input_size1, no_hidden_layers1, hidden_layer_size1, no_of_classes1, wt_initialisation1, optimiser1, activation_function1, batch_size1, eta1, epoch1, momentum1, beta1, beta11, beta21, loss_func1, lambd1, do_wandb_log1, plot_conf_mat1\n",
        "\n",
        "    nn = neural_network(layers, wt_initialisation)\n",
        "    if com == \"SGD\":\n",
        "      y_pred, y = nn.batch_grad_descent(x_train, y_train, x_val, y_val, no_of_classes, layers, activation_function, eta, batch_size, epoch, loss_func, lambd, do_wandb_log)\n",
        "    \n",
        "    if com == \"MOMENTUM\":\n",
        "      y_pred, y = nn.momentum_grad_descent(x_train, y_train, x_val, y_val, no_of_classes, layers, activation_function, batch_size, eta, epoch, momentum, loss_func, lambd, do_wandb_log)\n",
        "    if com == \"NAG\":\n",
        "      y_pred, y = nn.nesterov_gradient_descent_(x_train, y_train, x_val, y_val, no_of_classes, layers, activation_function, batch_size, eta, epoch, momentum, loss_func, lambd, do_wandb_log)\n",
        "\n",
        "    if com == \"RMSPROP\":\n",
        "      y_pred, y = nn.rmsprop_gradient_descent(x_train, y_train, x_val, y_val, no_of_classes, layers, activation_function, batch_size, eta, epoch, beta, loss_func, lambd, do_wandb_log)\n",
        "\n",
        "    if com == \"ADAM\":\n",
        "      y_pred, y = nn.adam_gradient_descent(x_train, y_train, x_val, y_val, no_of_classes, layers, activation_function, batch_size, eta, epoch, beta1, beta2, loss_func, lambd, do_wandb_log)\n",
        "\n",
        "    if com == \"NADAM\":\n",
        "      y_pred, y = nn.nadam_gradient_descent(x_train, y_train, x_val, y_val, no_of_classes, layers, activation_function, batch_size, eta, epoch, beta1, beta2, loss_func, lambd, do_wandb_log)\n",
        "\n",
        "    if plot_conf_mat == True: \n",
        "      y_pred = np.argmax(y_pred, axis = 1)\n",
        "      print(y_pred.shape, y.shape)\n",
        "      Confunsion_Matrix_Plot(y_pred, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HnFwkFiPf8h",
        "outputId": "40495408-0bec-4b7a-eff2-23bda3de9f30"
      },
      "outputs": [],
      "source": [
        "# main(x_train, y_train, x_val, y_val, input_size, no_hidden_layers, hidden_layer_size, no_of_classes, wt_initialisation, optimiser, activation_function, batch_size, eta, epoch, momentum, beta, beta1, beta2, loss_func, lambd, do_wandb_log, plot_conf_mat)\n",
        "# main(x_train, y_train, x_val, y_val, 784, 1, 128, 10, \"random\", \"momentum\", \"tanh\", 32, 0.0006, 3, 0.3, beta, beta1, beta2, \"cross_entropy\", 5, False, False)\n",
        "# eps in rms, adam, nadam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\bhati\\.netrc\n"
          ]
        }
      ],
      "source": [
        "!wandb login f6b40fc0bcc13c1d6117718b14ef1ddd1c68a700"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "id": "hbnKwBE1P0ft"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Data...\n",
            " Dataset Loaded !!\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading Data...\")\n",
        "dataset_name = 'fashion_mnist'\n",
        "if dataset_name == 'fashion_mnist':\n",
        "  (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "elif dataset_name == 'mnist':\n",
        "  (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1)\n",
        "\n",
        "print(\" Dataset Loaded !!\")\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "x_val = x_val / 255\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "x_val = x_val.reshape(x_val.shape[0], -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {},
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    \"name\": \"SWEEP Of Final Code\",\n",
        "    'method': 'random',\n",
        "    'metric': { 'goal': 'maximize','name': 'accuracy'},\n",
        "    'parameters': {\n",
        "                    'dataset' : {'values' : ['fashion_mnist']},\n",
        "                    'epochs': {'values': [3]},\n",
        "                    'batch_size': {'values': [16, 32, 64]},\n",
        "                    'loss': {'values': ['cross_entropy', 'mean_squared_error']}, # \"mean_squared_error\", \"cross_entropy\"\n",
        "                    'optimizer': {'values': [\"momentum\"]}, # \"sgd\", \"momentum\", \"nag\", \"rmsprop\", \"adam\", \"nadam\"\n",
        "                    'learning_rate': {'values': [1e-3, 1e-4]},\n",
        "                    'momentum': {'values': [0.5, 0.9]},\n",
        "                    'beta': {'values': [0.5, 0.9]},\n",
        "                    'beta1': {'values': [0.5, 0.9]},\n",
        "                    'beta2': {'values': [0.999]},\n",
        "                    'epsilon': {'values': [1e-5]},\n",
        "                    'weight_decay': {'values': [1e-8]},\n",
        "                    'weight_init': {'values': [\"random\", \"Xavier\"]}, # \"random\", \"Xavier\"\n",
        "                    'num_layers': {'values': [3, 4, 5]},\n",
        "                    'hidden_size': {'values': [32, 64, 128]},\n",
        "                    'activation': {'values': [\"sigmoid\"]}, # \"sigmoid\", \"tanh\", \"ReLU\"\n",
        "                    'input_size' : {'values': [ 784 ]}, # 784\n",
        "                    'output_size' : {'values': [ 10 ]}, # 10\n",
        "                    'wandb_log': {'values' : [True]}\n",
        "                  }\n",
        "               }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train( x_train, y_train, x_val, y_val):\n",
        "\n",
        "    parameter = wandb.init(project=\"DL_Assignment_1_final_code\")\n",
        "    config = parameter.config\n",
        "    wandb.run.name = (\n",
        "        \"o_\" + config.optimizer +\n",
        "        \"_e_\" + str(config.epochs) +\n",
        "        \"_af_\" + config.activation +\n",
        "        \"_wi_\" + config.weight_init +\n",
        "        \"_hl_\" + str(config.num_layers) +\n",
        "        \"_wd_\" + str(config.weight_decay) +\n",
        "        \"_m_\" + str( config.momentum) +\n",
        "        \"_eta_\" + str(config.learning_rate) +\n",
        "        \"_bs_\" + str(config.batch_size)\n",
        "    )\n",
        "\n",
        "    input_size =  config.input_size\n",
        "    no_hidden_layers = config.num_layers\n",
        "    hidden_layer_size = config.hidden_size\n",
        "    no_of_classes = config.output_size\n",
        "    wt_initialisation = config.weight_init\n",
        "    optimiser = config.optimizer\n",
        "    activation_function = config.activation\n",
        "    batch_size = config.batch_size\n",
        "    eta = config.learning_rate\n",
        "    epoch = config.epochs\n",
        "    momentum = config.momentum\n",
        "    beta = config.beta\n",
        "    beta1 = config.beta1\n",
        "    beta2 = config.beta2\n",
        "    loss_func = config.loss\n",
        "    lambd = config.weight_decay\n",
        "    do_wandb_log = config.wandb_log\n",
        "    layers = []\n",
        "    layers.append(input_size)\n",
        "    \n",
        "    i = 0\n",
        "    while i < no_hidden_layers:\n",
        "      layers.append(hidden_layer_size)\n",
        "      i += 1\n",
        "    layers.append(no_of_classes)\n",
        "\n",
        "    nn = NN(layers, wt_initialisation)\n",
        "    com = optimiser.upper()\n",
        "    if com == \"SGD\":\n",
        "      return nn.batch_grad_descent(x_train, y_train, x_val, y_val, no_of_classes, layers, activation_function, eta, batch_size, epoch, loss_func, lambd, do_wandb_log)\n",
        "    if com == \"MOMENTUM\":\n",
        "      return nn.momentum_grad_descent(x_train, y_train, x_val, y_val, no_of_classes, layers, activation_function, batch_size, eta, epoch, momentum, loss_func, lambd, do_wandb_log)\n",
        "    if com == \"NAG\":\n",
        "      return  nn.nesterov_gradient_descent_(x_train, y_train, x_val, y_val, no_of_classes, layers, activation_function, batch_size, eta, epoch, momentum, loss_func, lambd, do_wandb_log)\n",
        "\n",
        "    if com == \"RMSPROP\":\n",
        "      return nn.rmsprop_gradient_descent(x_train, y_train, x_val, y_val, no_of_classes, layers, activation_function, batch_size, eta, epoch, beta, loss_func, lambd, do_wandb_log)\n",
        "\n",
        "    if com == \"ADAM\":\n",
        "      return nn.adam_gradient_descent(x_train, y_train, x_val, y_val, no_of_classes, layers, activation_function, batch_size, eta, epoch, beta1, beta2, loss_func, lambd, do_wandb_log)\n",
        "\n",
        "    if com == \"NADAM\":\n",
        "      return nn.nadam_gradient_descent(x_train, y_train, x_val, y_val, no_of_classes, layers, activation_function, batch_size, eta, epoch, beta1, beta2, loss_func, lambd, do_wandb_log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: 9q4y2wuc\n",
            "Sweep URL: https://wandb.ai/cs23m074/DL_Assignment_1_final_code/sweeps/9q4y2wuc\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wir71d4g with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeta: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeta1: 0.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeta2: 0.999\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset: fashion_mnist\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon: 1e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_size: 784\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: momentum\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toutput_size: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twandb_log: True\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 1e-08\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_init: random\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.4"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>e:\\MTECH\\Semester 2\\Deep Learning Mitesh Khapra\\Assigment\\wandb\\run-20240317_210812-wir71d4g</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/cs23m074/DL_Assignment_1_final_code/runs/wir71d4g' target=\"_blank\">visionary-sweep-1</a></strong> to <a href='https://wandb.ai/cs23m074/DL_Assignment_1_final_code' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs23m074/DL_Assignment_1_final_code/sweeps/9q4y2wuc' target=\"_blank\">https://wandb.ai/cs23m074/DL_Assignment_1_final_code/sweeps/9q4y2wuc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/cs23m074/DL_Assignment_1_final_code' target=\"_blank\">https://wandb.ai/cs23m074/DL_Assignment_1_final_code</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/cs23m074/DL_Assignment_1_final_code/sweeps/9q4y2wuc' target=\"_blank\">https://wandb.ai/cs23m074/DL_Assignment_1_final_code/sweeps/9q4y2wuc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/cs23m074/DL_Assignment_1_final_code/runs/wir71d4g' target=\"_blank\">https://wandb.ai/cs23m074/DL_Assignment_1_final_code/runs/wir71d4g</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MOMENTUM\n",
            "1 Iteration No :  \t Train Loss\t 4.456447629081999\n",
            "1 Iteration No :  \t Validate Loss\t 4.457305567057291\n",
            "1 Iteration No :  \t Train Accuracy\t 9.97962962962963\n",
            "1 Iteration No :  \t Validate Accuracy\t 10.183333333333334\n",
            "---------------------------------------------------------\n",
            "MOMENTUM\n",
            "2 Iteration No :  \t Train Loss\t 4.397413945526264\n",
            "2 Iteration No :  \t Validate Loss\t 4.3984535700776854\n",
            "2 Iteration No :  \t Train Accuracy\t 9.97962962962963\n",
            "2 Iteration No :  \t Validate Accuracy\t 10.183333333333334\n",
            "---------------------------------------------------------\n",
            "MOMENTUM\n",
            "3 Iteration No :  \t Train Loss\t 4.342877585297955\n",
            "3 Iteration No :  \t Validate Loss\t 4.344086388465825\n",
            "3 Iteration No :  \t Train Accuracy\t 9.97962962962963\n",
            "3 Iteration No :  \t Validate Accuracy\t 10.183333333333334\n",
            "---------------------------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>train_accuracy</td><td>▁▁▁</td></tr><tr><td>train_error</td><td>█▄▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁</td></tr><tr><td>val_error</td><td>█▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>train_accuracy</td><td>9.97963</td></tr><tr><td>train_error</td><td>4.34288</td></tr><tr><td>val_accuracy</td><td>10.18333</td></tr><tr><td>val_error</td><td>4.34409</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">visionary-sweep-1</strong> at: <a href='https://wandb.ai/cs23m074/DL_Assignment_1_final_code/runs/wir71d4g' target=\"_blank\">https://wandb.ai/cs23m074/DL_Assignment_1_final_code/runs/wir71d4g</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20240317_210812-wir71d4g\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "  train(x_train, y_train, x_val, y_val)\n",
        "sweep_id = wandb.sweep(sweep_config, project = 'DL_Assignment_1_final_code', entity = \"cs23m074\")\n",
        "wandb.agent(sweep_id, main, count = 1)\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
