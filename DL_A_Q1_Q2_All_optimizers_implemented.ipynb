{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jgp5cd7fPk16",
        "outputId": "d5094994-3484-47ed-8645-d299549a9cd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wandb in c:\\users\\bhati\\anaconda3\\lib\\site-packages (0.16.4)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from wandb) (8.0.4)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from wandb) (3.1.42)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\bhati\\appdata\\roaming\\python\\python311\\site-packages (from wandb) (5.9.6)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from wandb) (1.41.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: setproctitle in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from wandb) (68.0.0)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from wandb) (4.23.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from Click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
            "Requirement already satisfied: six>=1.4.0 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\bhati\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Data...\n",
            " Dataset Loaded !!\n"
          ]
        }
      ],
      "source": [
        "#importing library as well loading datasets and spliting\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import wandb\n",
        "print(\"Loading Data...\")\n",
        "from keras.datasets import fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1)\n",
        "print(\" Dataset Loaded !!\")\n",
        "\n",
        "\n",
        "\n",
        "# importing only libraries\n",
        "\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import math\n",
        "# import matplotlib.pyplot as plt\n",
        "# import wandb\n",
        "# from keras.datasets import fashion_mnist,mnist\n",
        "# from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# normalizing values and reshaping training values\n",
        "\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "x_val = x_val / 255\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "x_val = x_val.reshape(x_val.shape[0], -1)\n",
        "\n",
        "#Global Variables\n",
        "beta = 0.9\n",
        "beta1 = 0.9\n",
        "beta2 = 0.999\n",
        "input_size = x_test.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-g_fAFBO_Sp",
        "outputId": "2bc2d663-ecf6-4fa3-b376-0cc6b77475fd"
      },
      "outputs": [],
      "source": [
        "class NN:\n",
        "  def __init__(self, s, weight_initialisation):\n",
        "    self.W, self.B, self.preactivation, self.activation = [],[],[],[]\n",
        "    if weight_initialisation.lower() != \"random\":\n",
        "      i = 1\n",
        "      lengtht = len(s)\n",
        "      while i < lengtht:\n",
        "        temp = 6 / (s[i] + s[i-1])\n",
        "        n = np.sqrt(temp)\n",
        "        w = np.random.uniform(-n , n, (s[i], s[i-1]))\n",
        "        self.W.append(w)\n",
        "        b = np.random.uniform(-n , n, (s[i]))\n",
        "        self.B.append(b)\n",
        "        i += 1\n",
        "    # Random weight Initialisation\n",
        "    elif weight_initialisation.upper() != \"XAVIER\":\n",
        "      i = 1\n",
        "      while i < len(s):\n",
        "        b = np.random.randn(s[i])\n",
        "        self.B.append(b)\n",
        "        w = np.random.randn(s[i], s[i-1]) /(np.sqrt(s[i]))\n",
        "        self.W.append(w)\n",
        "        i += 1\n",
        "\n",
        "\n",
        "  # not normalizing\n",
        "\n",
        "  def sigmoid(self, x):\n",
        "    temp2 = np.exp(-x)\n",
        "    temp = 1.0/(1.0 + temp2)\n",
        "    return temp\n",
        "\n",
        "\n",
        "\n",
        "  def sigmoid_derivative(self, x):\n",
        "    sigm = self.sigmoid(x)\n",
        "    temp = sigm\n",
        "    temp2 = 1 - sigm \n",
        "    return sigm*(1-sigm)\n",
        "\n",
        "\n",
        "  def relu(self, x):\n",
        "    return np.where(x > 0, x, 0)\n",
        "\n",
        "\n",
        "  def relu_derivative(self, x):\n",
        "    return np.where(x < 0, 0, 1)\n",
        "\n",
        "\n",
        "\n",
        "  def tanh(self, x):\n",
        "    t = np.tanh(x)\n",
        "    return t \n",
        "\n",
        "\n",
        "  def tanh_derivative(self, x):\n",
        "    temp = np.tanh(x)\n",
        "    temp2 = 1 - temp*temp\n",
        "    return temp2\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "  def softmax(self, x):\n",
        "    i = 0\n",
        "    while i < x.shape[0]:\n",
        "      argmax = np.argmax(x[i])\n",
        "      sum = 0\n",
        "      maxval = x[i][argmax]\n",
        "      j = 0\n",
        "      while j < x.shape[1]:\n",
        "        sum = sum + np.exp(x[i][j]-maxval)\n",
        "        j += 1\n",
        "      x[i] = np.exp(x[i]-maxval)/sum\n",
        "      i+=1\n",
        "    return x\n",
        "  \n",
        "  def softmax_derivative(self, x):\n",
        "    temp = self.softmax(x)\n",
        "    temp2 = 1 - temp\n",
        "    return temp*temp2\n",
        "\n",
        "\n",
        "  def one_hot_encoded(self, y, size):\n",
        "    temp = np.eye(size)[y]\n",
        "    return temp\n",
        "\n",
        "\n",
        "  def cross_entropy(self, y_train, y_hat):\n",
        "    loss = 0\n",
        "    i = 0\n",
        "    while i < y_hat.shape[0]:\n",
        "        loss += -(np.log2(y_hat[i][y_train[i]]))\n",
        "        i += 1\n",
        "    return loss/y_hat.shape[0]\n",
        "\n",
        "\n",
        "\n",
        "  def squared_error(self, y_train, y_hat, no_of_classes):\n",
        "      y_onehot = self.one_hot_encoded(y_train, no_of_classes)\n",
        "      loss = 0\n",
        "      i = 0\n",
        "      while i < y_hat.shape[0]:\n",
        "          loss += np.sum((y_hat[i] - y_onehot[i])**2)\n",
        "          i+=1\n",
        "      return loss / y_train.shape[0]\n",
        "\n",
        "  def loss_function(self, y_train, y_hat, no_of_classes, loss_func, lambd):\n",
        "    temp = y_train.shape[0]\n",
        "    loss = self.l2_regularize(lambd, temp)\n",
        "    if loss_func.upper() != \"CROSS_ENTROPY\":\n",
        "      loss = loss + self.squared_error(y_train, y_hat, no_of_classes)\n",
        "    else:\n",
        "      loss = loss + self.cross_entropy(y_train, y_hat)\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "  # hadmard gone\n",
        "\n",
        "\n",
        "  def l2_regularize(self, lambd, batch_size):\n",
        "      acc = 0\n",
        "      i = 0\n",
        "      while i < len(self.W):\n",
        "        acc += np.sum(self.W[i] ** 2)\n",
        "        i += 1\n",
        "      temp = (lambd/(2.* batch_size)) * acc\n",
        "      return temp\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def batch_converter(self, x1, y1, batch_size1):\n",
        "    x, y, batch_size = x1, y1, batch_size1\n",
        "    x_batch = []\n",
        "    y_batch = []\n",
        "    num_datapoints = x.shape[0]\n",
        "    no_datapoints = num_datapoints\n",
        "    no_batches = no_datapoints // batch_size                   # floor division operator\n",
        "    i = 0\n",
        "    while i < no_batches:\n",
        "      e = 0\n",
        "      if (i+1)*batch_size < x.shape[0]:\n",
        "        e = (i+1)*batch_size\n",
        "      else:\n",
        "        e = x.shape[0]\n",
        "      s = i*batch_size\n",
        "      x1 = np.array(x[s:e])        # slicing\n",
        "      y1 = np.array(y[s:e])        # slicing\n",
        "      x_batch.append(x1)\n",
        "      y_batch.append(y1)\n",
        "      i += 1\n",
        "    # jo datapoints last me bach jayenge wo yaha pe add kr rhe\n",
        "    temp = no_batches * batch_size\n",
        "    if temp != x_train.shape[0]:\n",
        "      x1 = np.array(x_train[temp :])\n",
        "      y1 = np.array(y_train[temp :])\n",
        "      x_batch.append(x1)\n",
        "      y_batch.append(y1)\n",
        "    return x_batch, y_batch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, input, size1, activation_function1):\n",
        "    # Calculating for the hiddlen layers\n",
        "    size, activation_function = size1, activation_function1\n",
        "    i = 0\n",
        "    temp2 = activation_function.upper()\n",
        "    temp = len(size)-1\n",
        "    while i < temp:\n",
        "          Y = np.dot(input, self.W[i].T) + self.B[i]\n",
        "          \n",
        "          '''Not normalizing'''\n",
        "          if i < len(size)-2:\n",
        "            if i < len(self.preactivation):\n",
        "              self.preactivation[i] = Y\n",
        "            else:\n",
        "              self.preactivation.append(Y)\n",
        "            # Y_dash = self.normalize(Y)\n",
        "            if temp2 == \"RELU\":\n",
        "              Z = self.relu(Y)\n",
        "            if temp2 ==\"SIGMOID\":\n",
        "              Z = self.sigmoid(Y)\n",
        "            if temp2 ==\"TANH\":\n",
        "              Z = self.tanh(Y)\n",
        "            \n",
        "            temp3 = len(self.activation)\n",
        "            if i < temp3:\n",
        "              self.activation[i] = Z\n",
        "            else:\n",
        "              self.activation.append(Z)\n",
        "            input = Z\n",
        "          else:\n",
        "            #Calculating for the output layer.\n",
        "            Y = np.dot(input, self.W[i].T) + self.B[i]\n",
        "            # Y = self.normalize(Y)\n",
        "            temp4 =len(self.preactivation)\n",
        "            if i < temp4:\n",
        "              self.preactivation[i] = Y\n",
        "            else:\n",
        "              self.preactivation.append(Y)\n",
        "            Z = self.softmax(Y)\n",
        "            temp5 = len(self.activation)\n",
        "            if i < temp5:\n",
        "              self.activation[i] = Z\n",
        "            else:\n",
        "              self.activation.append(Z)\n",
        "          i = i + 1\n",
        "    return self.preactivation, self.activation\n",
        "\n",
        "\n",
        "  def backward(self, layers1, x1, y1 ,no_of_classes1, preac1, ac1, activation_function1, loss_func1):\n",
        "    layers, x, y ,no_of_classes, preac, ac, activation_function, loss_func = layers1, x1, y1 ,no_of_classes1, preac1, ac1, activation_function1, loss_func1\n",
        "    no_layers = len(layers)\n",
        "    grad_a, grad_w, grad_b, grad_h = [],[],[],[]\n",
        "    y_onehot = self.one_hot_encoded(y, no_of_classes)\n",
        "    temp4 = activation_function.upper()\n",
        "    temp = \"cross_entropy\"\n",
        "    temp2 = len(ac)-1\n",
        "    if loss_func.lower() == temp:\n",
        "\n",
        "      temp = -(y_onehot - ac[temp2])\n",
        "      grad_a.append(temp)\n",
        "    else: #MSE\n",
        "      grad_a.append((ac[temp2] - y_onehot) * self.softmax_derivative(ac[temp2]))#ac[len(ac)-1]*(1 - ac[len(ac)-1]))\n",
        "    i = no_layers - 2\n",
        "    while i  > -1:\n",
        "      temp3 = no_layers-2-i\n",
        "      if i == 0:\n",
        "        dw = (grad_a[temp3].T @ x) #/ y.shape[0]\n",
        "        db = np.sum(grad_a[temp3],axis=0)/y.shape[0]\n",
        "      elif i > 0:\n",
        "        dw = (grad_a[temp3].T @ ac[i-1])#/ y.shape[0]\n",
        "        db = np.sum(grad_a[temp3],axis=0)/ y.shape[0]\n",
        "        dh_1 = grad_a[temp3] @ self.W[i]\n",
        "        sig = 0\n",
        "        if temp4 == \"SIGMOID\":\n",
        "          sig = self.sigmoid_derivative(preac[i-1])\n",
        "        if temp4 == \"RELU\":\n",
        "          sig = self.relu_derivative(preac[i-1])\n",
        "        if temp4 == \"TANH\":\n",
        "          sig = self.tanh_derivative(preac[i-1])\n",
        "        \n",
        "\n",
        "        da_1 = dh_1 * sig\n",
        "\n",
        "        grad_h.append(dh_1)\n",
        "        grad_a.append(da_1)\n",
        "      grad_b.append(db)\n",
        "      grad_w.append(dw)\n",
        "      i -= 1\n",
        "    return grad_w, grad_b\n",
        "\n",
        "\n",
        "  def printingFunction(self, loss_train, loss_val, accur_train, accur_val, i, optimizer_name):\n",
        "      print(optimizer_name)\n",
        "      print(i+1, \"Iteration No : \", \"\\t Train Loss\\t\", loss_train)\n",
        "      print(i+1, \"Iteration No : \", \"\\t Validate Loss\\t\", loss_val)\n",
        "      print(i+1, \"Iteration No : \", \"\\t Train Accuracy\\t\", accur_train)\n",
        "      print(i+1, \"Iteration No : \", \"\\t Validate Accuracy\\t\", accur_val)\n",
        "      print(\"---------------------------------------------------------\")\n",
        "\n",
        "  def batch_grad_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, activation_function, eta, batch_size, n_iterations, loss_func, lambd, do_wandb_log):\n",
        "    x_batch, y_batch = self.batch_converter(x_train, y_train, batch_size)\n",
        "    loss_arr = []\n",
        "    i = 0\n",
        "    length = len(layers)\n",
        "    while i < n_iterations:\n",
        "      j = 0\n",
        "      while j < len(x_batch):\n",
        "        xb, yb = x_batch[j], y_batch[j]\n",
        "        preac, ac = None, None\n",
        "        def f(xb, layers, activation_function, yb, no_of_classes, preac, ac, loss_func):\n",
        "          preac, ac = self.forward(xb, layers, activation_function)\n",
        "          grad_w, grad_b = self.backward(layers, xb, yb, no_of_classes,preac, ac, activation_function, loss_func)\n",
        "          return preac, ac, grad_w, grad_b\n",
        "        preac, ac, grad_w, grad_b = f(xb, layers, activation_function, yb, no_of_classes, preac, ac, loss_func)\n",
        "        l = 0\n",
        "        while l < length-1:\n",
        "          # print(\"shape\",self.W[l].shape, grad_w[length-l-2].shape)\n",
        "          self.W[l] += -(eta * grad_w[length-l-2] + eta * lambd * self.W[l])\n",
        "          self.B[l] += -eta * grad_b[length-l-2]\n",
        "          l += 1\n",
        "        j += 1\n",
        "\n",
        "      loss_train = 0\n",
        "      loss_val = 0\n",
        "\n",
        "      def f2(x_train, layers, activation_function, y_train, ac, no_of_classes, loss_func, lambd, x_test, y_test):\n",
        "        preac, ac = self.forward(x_train, layers, activation_function)\n",
        "        loss_train = self.loss_function(y_train, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "\n",
        "        preac, ac = self.forward(x_test, layers, activation_function )\n",
        "        loss_val = self.loss_function(y_test, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "        return loss_train, loss_val\n",
        "\n",
        "      loss_train, loss_val = f2(x_train, layers, activation_function, y_train, ac, no_of_classes, loss_func, lambd, x_test, y_test)\n",
        "      loss_arr.append(loss_val)\n",
        "      accur_train = self.test_accuracy(layers, x_train, y_train, activation_function)\n",
        "      accur_val = self.test_accuracy(layers, x_test, y_test, activation_function)\n",
        "\n",
        "      name = \"SGD\"\n",
        "      self.printingFunction(loss_train, loss_val, accur_train, accur_val, i, name)\n",
        "\n",
        "\n",
        "      # print(i+1, \"Iteration No : \\t\\t\", \"\\t Train Loss\\t\\t\", loss_train)\n",
        "      # print(i+1, \"Iteration No : \\t\\t\", \"\\t Validate Loss\\t\\t\\n\", loss_val)\n",
        "      # print(i+1, \"Iteration No : \\t\\t\", \"\\t Train Accuracy\\t\\t\", accur_train)\n",
        "      # print(i+1, \"Iteration No : \\t\\t\", \"\\t Validate Accuracy\\t\\t\", accur_val)\n",
        "      # print(\"---------------------------------------------------------\")\n",
        "      if do_wandb_log == True:\n",
        "        wandb.log({\"epoch\": ep+1 , \"train_accuracy\":accur_train,\"train_error\":loss_train,\"val_accuracy\":accur_val,\"val_error\":loss_val})\n",
        "      i += 1\n",
        "    return ac[len(ac)-1], y_test\n",
        "\n",
        "\n",
        "\n",
        "  def momentum_grad_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, activation_function, batch_size, eta, epochs, beta, loss_func,lambd, do_wandb_log):\n",
        "    l = len(layers)\n",
        "    prev_w, prev_b, loss_arr = [],[],[]\n",
        "    i = 0\n",
        "    while i < l-1:\n",
        "      prev_w.append(np.zeros(self.W[i].shape))\n",
        "      prev_b.append(np.zeros(self.B[i].shape))\n",
        "      i += 1\n",
        "    x_batch, y_batch = self.batch_converter(x_train, y_train, batch_size)\n",
        "    ep = 0\n",
        "    while ep < epochs:\n",
        "      j = 0\n",
        "      while j < len(x_batch):\n",
        "        xb = x_batch[j]\n",
        "        yb = y_batch[j]\n",
        "        preac, ac = self.forward(xb, layers, activation_function)\n",
        "        grad_w, grad_b = self.backward(layers, xb, yb, no_of_classes, preac, ac, activation_function, loss_func)\n",
        "        i = 0\n",
        "        while i < l-1:\n",
        "          prev_w[i] = beta*prev_w[i] + grad_w[l-i-2]\n",
        "          prev_b[i] = beta*prev_b[i] + grad_b[l-i-2]\n",
        "          self.W[i] += -(eta*prev_w[i] + eta * lambd * self.W[i])\n",
        "          self.B[i] += -eta*prev_b[i]\n",
        "          i += 1\n",
        "        j += 1\n",
        "\n",
        "      loss_train = 0\n",
        "      loss_val = 0\n",
        "\n",
        "      def f3(x_train, layers, activation_function, y_train, ac, no_of_classes, loss_func, lambd, x_test, y_test):\n",
        "\n",
        "        preac, ac = self.forward(x_train, layers, activation_function)\n",
        "        loss_train = self.loss_function(y_train, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "\n",
        "        preac, ac = self.forward(x_test, layers, activation_function)\n",
        "        loss_val = self.loss_function(y_test, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "        return loss_train, loss_val\n",
        "\n",
        "      loss_train, loss_val = f3(x_train, layers, activation_function, y_train, ac, no_of_classes, loss_func, lambd, x_test, y_test)\n",
        "      loss_arr.append(loss_val)\n",
        "      accur_train = self.test_accuracy(layers, x_train, y_train, activation_function)\n",
        "      accur_val = self.test_accuracy(layers, x_test, y_test, activation_function)\n",
        "\n",
        "\n",
        "\n",
        "      name = \"MOMENTUM\"\n",
        "      self.printingFunction(loss_train, loss_val, accur_train, accur_val, ep, name)\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "      if do_wandb_log == True:\n",
        "        wandb.log({\"epoch\": ep+1 , \"train_accuracy\":accur_train,\"train_error\":loss_train,\"val_accuracy\":accur_val,\"val_error\":loss_val})\n",
        "      ep += 1\n",
        "    return ac[len(ac)-1], y_test\n",
        "\n",
        "\n",
        "\n",
        "  def nesterov_gradient_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, activation_function, batch_size, eta, epochs, beta, loss_func,lambd, do_wandb_log):\n",
        "    l = len(layers)\n",
        "    prev_w, prev_b, loss_arr = [],[],[]\n",
        "    i = 0\n",
        "    while i < l-1:\n",
        "      prev_w.append(np.zeros(self.W[i].shape))\n",
        "      prev_b.append(np.zeros(self.B[i].shape))\n",
        "      i += 1\n",
        "    x_batch, y_batch = self.batch_converter(x_train, y_train, batch_size)\n",
        "\n",
        "    ep = 0\n",
        "    while ep < epochs:\n",
        "      j = 0\n",
        "      while j < len(x_batch):\n",
        "        xb = x_batch[j]\n",
        "        yb = y_batch[j]\n",
        "        i = 0\n",
        "        while i < l-1:\n",
        "          self.W[i] += -beta * prev_w[i]\n",
        "          self.B[i] += -beta * prev_b[i]\n",
        "          i += 1\n",
        "        preac, ac = self.forward(xb, layers, activation_function)\n",
        "        grad_w, grad_b = self.backward(layers, xb, yb, no_of_classes, preac, ac, activation_function, loss_func)\n",
        "        # print(\"grad_w\", grad_w)\n",
        "        i = 0\n",
        "        while i < l-1:\n",
        "          prev_w[i] = beta * prev_w[i] + grad_w[l-i-2]\n",
        "          prev_b[i] = beta * prev_b[i] + grad_b[l-i-2]\n",
        "          self.W[i] += -(eta * prev_w[i] + eta * lambd * self.W[i])\n",
        "          self.B[i] += -eta * prev_b[i]\n",
        "          i += 1\n",
        "        j += 1\n",
        "\n",
        "\n",
        "      loss_train = 0\n",
        "      loss_val = 0\n",
        "\n",
        "      def f4(x_train, layers, activation_function, y_train, ac, no_of_classes, loss_func, lambd, x_test, y_test):\n",
        "\n",
        "        preac, ac = self.forward(x_train, layers, activation_function)\n",
        "        loss_train = self.loss_function(y_train, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "\n",
        "        preac, ac = self.forward(x_test, layers, activation_function)\n",
        "        loss_val = self.loss_function(y_test, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "        return loss_train, loss_val\n",
        "\n",
        "      loss_train, loss_val = f4(x_train, layers, activation_function, y_train, ac, no_of_classes, loss_func, lambd, x_test, y_test)\n",
        "      loss_arr.append(loss_val)\n",
        "      accur_train = self.test_accuracy(layers, x_train, y_train, activation_function)\n",
        "      accur_val = self.test_accuracy(layers, x_test, y_test, activation_function)\n",
        "\n",
        "\n",
        "\n",
        "      name = \"NAG\"\n",
        "      self.printingFunction(loss_train, loss_val, accur_train, accur_val, ep, name)\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "      if do_wandb_log == True:\n",
        "        wandb.log({\"epoch\": ep+1 , \"train_accuracy\":accur_train,\"train_error\":loss_train,\"val_accuracy\":accur_val,\"val_error\":loss_val})\n",
        "      ep += 1\n",
        "    return ac[len(ac)-1], y_test\n",
        "\n",
        "\n",
        "  def rmsprop_gradient_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, activation_function, batch_size, eta, epochs, beta, loss_func,lambd, do_wandb_log):\n",
        "    eps = 1e-4\n",
        "    vw, vb, loss_arr = [],[],[]\n",
        "    l = len(layers)\n",
        "    i = 0\n",
        "    while i < l-1:\n",
        "      vw.append(np.zeros(self.W[i].shape))\n",
        "      vb.append(np.zeros(self.B[i].shape))\n",
        "      i += 1\n",
        "\n",
        "    x_batch, y_batch = self.batch_converter(x_train, y_train, batch_size)\n",
        "    ep = 0\n",
        "    while ep < epochs:\n",
        "      j = 0\n",
        "      while j < len(x_batch):\n",
        "        xb = x_batch[j]\n",
        "        yb = y_batch[j]\n",
        "        preac, ac = self.forward(xb, layers, activation_function)\n",
        "        grad_w, grad_b = self.backward(layers, xb, yb, no_of_classes, preac, ac, activation_function, loss_func)\n",
        "        i = 0\n",
        "        while i < l-2:\n",
        "          vw[i] = beta * vw[i] + (1-beta) * grad_w[l-i-2] * grad_w[l-i-2]\n",
        "          vb[i] = beta * vb[i] + (1-beta) * grad_b[l-i-2] * grad_b[l-i-2]\n",
        "          self.W[i] =  self.W[i] - (eta * grad_w[l-i-2])/np.sqrt(vw[i] + eps) - (eta * lambd * self.W[i])\n",
        "          self.B[i] =  self.B[i] - (eta * grad_b[l-i-2])/np.sqrt(vb[i] + eps)\n",
        "          i += 1\n",
        "        vw[i] = beta * vw[i] + (1-beta) * grad_w[l-i-2] * grad_w[l-i-2]\n",
        "        vb[i] = beta * vb[i] + (1-beta) * grad_b[l-i-2] * grad_b[l-i-2]\n",
        "        self.W[i] =  self.W[i] - (eta * grad_w[l-i-2])/np.sqrt(vw[i] + eps) - (eta * lambd * self.W[i])\n",
        "        self.B[i] =  self.B[i] - (eta * grad_b[l-i-2])/np.sqrt(vb[i] + eps)\n",
        "        j += 1\n",
        "\n",
        "\n",
        "      loss_train = 0\n",
        "      loss_val = 0\n",
        "\n",
        "      def f5(x_train, layers, activation_function, y_train, ac, no_of_classes, loss_func, lambd, x_test, y_test):\n",
        "\n",
        "        preac, ac = self.forward(x_train, layers, activation_function)\n",
        "        loss_train = self.loss_function(y_train, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "\n",
        "        preac, ac = self.forward(x_test, layers, activation_function)\n",
        "        loss_val = self.loss_function(y_test, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "        return loss_train, loss_val\n",
        "\n",
        "      loss_train, loss_val = f5(x_train, layers, activation_function, y_train, ac, no_of_classes, loss_func, lambd, x_test, y_test)\n",
        "      loss_arr.append(loss_val)\n",
        "      accur_train = self.test_accuracy(layers, x_train, y_train, activation_function)\n",
        "      accur_val = self.test_accuracy(layers, x_test, y_test, activation_function)\n",
        "\n",
        "      \n",
        "      name = \"RMSPROP\"\n",
        "      self.printingFunction(loss_train, loss_val, accur_train, accur_val, ep, name)\n",
        "\n",
        "\n",
        "      if do_wandb_log == True:\n",
        "        wandb.log({\"epoch\": ep+1 , \"train_accuracy\":accur_train,\"train_error\":loss_train,\"val_accuracy\":accur_val,\"val_error\":loss_val})\n",
        "      ep += 1\n",
        "    return ac[len(ac)-1], y_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def adam_gradient_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, activation_function, batch_size, eta, epochs, beta1, beta2, loss_func,lambd, do_wandb_log):\n",
        "    l = len(layers)\n",
        "    mw, mb, vw, vb = [],[],[],[]\n",
        "    eps = 1e-10\n",
        "    loss_arr = []\n",
        "    i = 0\n",
        "    while i < l-1:\n",
        "      vb.append(np.zeros(self.B[i].shape))\n",
        "      mw.append(np.zeros(self.W[i].shape))\n",
        "      vw.append(np.zeros(self.W[i].shape))\n",
        "      mb.append(np.zeros(self.B[i].shape))\n",
        "      i += 1\n",
        "    x_batch, y_batch = self.batch_converter(x_train, y_train, batch_size)\n",
        "    ep = 0\n",
        "    while ep < epochs:\n",
        "      j = 0\n",
        "      while j < len(x_batch):\n",
        "        xb = x_batch[j]\n",
        "        yb = y_batch[j]\n",
        "        preac, ac = self.forward(xb, layers, activation_function)\n",
        "        grad_w, grad_b = self.backward(layers, xb, yb, no_of_classes, preac, ac, activation_function, loss_func)\n",
        "        i = 0\n",
        "        while i < l-1:\n",
        "          temp = l-i-2\n",
        "          mw[i] = beta1 * mw[i] + (1-beta1) * grad_w[temp]\n",
        "          mb[i] = beta1 * mb[i] + (1-beta1) * grad_b[temp]\n",
        "          mw_hat = mw[i] / (1 - np.power(beta1, j+1))\n",
        "          mb_hat = mb[i] / (1 - np.power(beta1, j+1))\n",
        "\n",
        "          vw[i] = beta2 * vw[i] + (1-beta2) * grad_w[temp] * grad_w[temp]\n",
        "          vb[i] = beta2 * vb[i] + (1-beta2) * grad_b[temp] * grad_b[temp]\n",
        "          vw_hat = vw[i] / (1 - np.power(beta2, j+1))\n",
        "          vb_hat = vb[i] / (1 - np.power(beta2, j+1))\n",
        "\n",
        "          self.W[i] = self.W[i] - (eta * mw_hat)/(np.sqrt(vw_hat) + eps)- (eta * lambd * self.W[i])\n",
        "          self.B[i] = self.B[i] - (eta * mb_hat)/(np.sqrt(vb_hat) + eps)\n",
        "          i += 1\n",
        "        j += 1\n",
        "      preac, ac = self.forward(x_train, layers, activation_function)\n",
        "      loss_train = self.loss_function(y_train, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "\n",
        "      preac, ac = self.forward(x_test, layers, activation_function)\n",
        "      loss_val = self.loss_function(y_test, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "\n",
        "      loss_arr.append(loss_val)\n",
        "      accur_train = self.test_accuracy(layers, x_train, y_train, activation_function)\n",
        "      accur_val = self.test_accuracy(layers, x_test, y_test, activation_function)\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "      name = \"ADAM\"\n",
        "      self.printingFunction(loss_train, loss_val, accur_train, accur_val, ep, name)\n",
        "\n",
        "\n",
        "      if do_wandb_log == True:\n",
        "        wandb.log({\"epoch\": ep+1 , \"train_accuracy\":accur_train,\"train_error\":loss_train,\"val_accuracy\":accur_val,\"val_error\":loss_val})\n",
        "      ep += 1\n",
        "    return ac[len(ac)-1], y_test\n",
        "\n",
        "\n",
        "  def nadam_gradient_descent(self, x_train, y_train, x_test, y_test, no_of_classes, layers, activation_function, batch_size, eta, epochs, beta1, beta2, loss_func,lambd, do_wandb_log):\n",
        "    eps = 1e-10\n",
        "    mw, mb, vw, vb = [],[],[],[]\n",
        "    x_batch, y_batch = self.batch_converter(x_train, y_train, batch_size)\n",
        "    l = len(layers)\n",
        "    loss_arr = []\n",
        "    i = 0\n",
        "    while i < l-2:\n",
        "      vw.append(np.zeros(self.W[i].shape))\n",
        "      vb.append(np.zeros(self.B[i].shape))\n",
        "      mw.append(np.zeros(self.W[i].shape))\n",
        "      mb.append(np.zeros(self.B[i].shape))\n",
        "      i += 1\n",
        "    vw.append(np.zeros(self.W[i].shape))\n",
        "    vb.append(np.zeros(self.B[i].shape))\n",
        "    mw.append(np.zeros(self.W[i].shape))\n",
        "    mb.append(np.zeros(self.B[i].shape))\n",
        "    x_batch, y_batch = self.batch_converter(x_train, y_train, batch_size)\n",
        "    ep = 0\n",
        "    while ep < epochs:\n",
        "      j = 0\n",
        "      while j < len(x_batch):\n",
        "        xb = x_batch[j]\n",
        "        yb = y_batch[j]\n",
        "        preac, ac = self.forward(xb, layers, activation_function)\n",
        "        grad_w, grad_b = self.backward(layers, xb, yb, no_of_classes, preac, ac, activation_function, loss_func)\n",
        "        i = 0\n",
        "        while i < l-2:\n",
        "          mw[i] = beta1 * mw[i] + (1-beta1)* grad_w[l-i-2]\n",
        "          mb[i] = beta1 * mb[i] + (1-beta1)* grad_b[l-i-2]\n",
        "          mw_hat = mw[i] / (1 - np.power(beta1, j+1))\n",
        "          mb_hat = mb[i] / (1 - np.power(beta1, j+1))\n",
        "\n",
        "          vw[i] = beta2 * vw[i] + (1-beta2) * grad_w[l-i-2] * grad_w[l-i-2]\n",
        "          vb[i] = beta2 * vb[i] + (1-beta2) * grad_b[l-i-2] * grad_b[l-i-2]\n",
        "          vw_hat = vw[i] / (1 - np.power(beta2, j+1))\n",
        "          vb_hat = vb[i] / (1 - np.power(beta2, j+1))\n",
        "\n",
        "          self.W[i] = self.W[i] - (eta/(np.sqrt(vw[i]) + eps)) * (beta1 * mw_hat + (((1-beta1) * grad_w[l-i-2]) / (1 - np.power(beta1, j+1)))) - (eta * lambd * self.W[i])\n",
        "          self.B[i] = self.B[i] - (eta/(np.sqrt(vb[i]) + eps)) * (beta1 * mb_hat + (((1-beta1) * grad_b[l-i-2]) / (1 - np.power(beta1, j+1))))\n",
        "          i += 1\n",
        "        mw[i] = beta1 * mw[i] + (1-beta1)* grad_w[l-i-2]\n",
        "        mb[i] = beta1 * mb[i] + (1-beta1)* grad_b[l-i-2]\n",
        "        mw_hat = mw[i] / (1 - np.power(beta1, j+1))\n",
        "        mb_hat = mb[i] / (1 - np.power(beta1, j+1))\n",
        "\n",
        "        vw[i] = beta2 * vw[i] + (1-beta2) * grad_w[l-i-2] * grad_w[l-i-2]\n",
        "        vb[i] = beta2 * vb[i] + (1-beta2) * grad_b[l-i-2] * grad_b[l-i-2]\n",
        "        vw_hat = vw[i] / (1 - np.power(beta2, j+1))\n",
        "        vb_hat = vb[i] / (1 - np.power(beta2, j+1))\n",
        "\n",
        "        self.W[i] = self.W[i] - (eta/(np.sqrt(vw[i]) + eps)) * (beta1 * mw_hat + (((1-beta1) * grad_w[l-i-2]) / (1 - np.power(beta1, j+1)))) - (eta * lambd * self.W[i])\n",
        "        self.B[i] = self.B[i] - (eta/(np.sqrt(vb[i]) + eps)) * (beta1 * mb_hat + (((1-beta1) * grad_b[l-i-2]) / (1 - np.power(beta1, j+1))))\n",
        "        j += 1\n",
        "\n",
        "      loss_train = 0\n",
        "      loss_val = 0\n",
        "\n",
        "      def f5(x_train, layers, activation_function, y_train, ac, no_of_classes, loss_func, lambd, x_test, y_test):\n",
        "\n",
        "        preac, ac = self.forward(x_train, layers, activation_function)\n",
        "        loss_train = self.loss_function(y_train, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "\n",
        "        preac, ac = self.forward(x_test, layers, activation_function)\n",
        "        loss_val = self.loss_function(y_test, ac[len(ac)-1], no_of_classes, loss_func, lambd)\n",
        "        return loss_train, loss_val\n",
        "\n",
        "      loss_train, loss_val = f5(x_train, layers, activation_function, y_train, ac, no_of_classes, loss_func, lambd, x_test, y_test)\n",
        "      loss_arr.append(loss_val)\n",
        "      accur_train = self.test_accuracy(layers, x_train, y_train, activation_function)\n",
        "      accur_val = self.test_accuracy(layers, x_test, y_test, activation_function)\n",
        "\n",
        "      # print(ep+1, \"Iteration No : \\t\\t\", \"\\t Train Loss\\t\\t\", loss_train)\n",
        "      # print(ep+1, \"Iteration No : \\t\\t\", \"\\t Validate Loss\\t\\t\", loss_val, \"\\n\")\n",
        "\n",
        "\n",
        "      # print(ep+1, \"Iteration No : \\t\\t\", \"\\t Train Accuracy\\t\\t\", accur_train)\n",
        "      # print(ep+1,\"Iteration No : \\t\\t\", \"\\t Validate Accuracy\\t\\t\", accur_val)\n",
        "      # print(\"---------------------------------------------------------\")\n",
        "      name = \"NADAM\"\n",
        "      self.printingFunction(loss_train, loss_val, accur_train, accur_val, ep, name)\n",
        "\n",
        "      if do_wandb_log == True:\n",
        "        wandb.log({\"epoch\": ep+1 , \"train_accuracy\":accur_train,\"train_error\":loss_train,\"val_accuracy\":accur_val,\"val_error\":loss_val})\n",
        "      ep += 1\n",
        "    return ac[len(ac)-1], y_test\n",
        "\n",
        "\n",
        "  def test_accuracy(self, layers1, x1, y1, activation_function1):\n",
        "    layers, x, y, activation_function = layers1, x1, y1, activation_function1\n",
        "    preac, ac = self.forward(x, layers, activation_function)\n",
        "    y_pred = ac[len(ac)-1]\n",
        "    err_count = 0\n",
        "    i = 0\n",
        "    while i < y_pred.shape[0]:\n",
        "      maxval = -(math.inf)\n",
        "      maxind = -1\n",
        "      j = 0\n",
        "      while j < y_pred.shape[1]:\n",
        "        if maxval < y_pred[i][j]:\n",
        "          maxval = y_pred[i][j]\n",
        "          maxind = j\n",
        "        j += 1\n",
        "      if maxind != y[i]:\n",
        "        err_count = err_count + 1\n",
        "      i += 1\n",
        "    temp = ((y.shape[0] - err_count)/y.shape[0])*100\n",
        "    return temp\n",
        "    \n",
        "  def PlotError(self, ErrorSum):\n",
        "    Iter = []\n",
        "    for i in range(len(ErrorSum)):\n",
        "      Iter.append(i)\n",
        "    plt.plot(Iter,ErrorSum)\n",
        "    plt.title('Error v/s Iteration')\n",
        "    plt.xlabel('No of Iterations')\n",
        "    plt.ylabel('Error')\n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "  \n",
        "      \n",
        "\n",
        "      \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Cuhbg19NPiKD"
      },
      "outputs": [],
      "source": [
        "def main(x_train1, y_train1, x_val1, y_val1, input_size1, no_hidden_layers1, hidden_layer_size1, no_of_classes1, wt_initialisation1, optimiser1, activation_function1, batch_size1, eta1, epoch1, momentum1, beta1, beta11, beta21, loss_func1, lambd1, do_wandb_log1, plot_conf_mat1):\n",
        "    com = optimiser1.upper() \n",
        "    layers = []\n",
        "    layers.append(input_size1)\n",
        "    i = 0\n",
        "    while i < no_hidden_layers1:\n",
        "      layers.append(hidden_layer_size1)\n",
        "      i+=1\n",
        "    layers.append(no_of_classes1)\n",
        "\n",
        "    x_train, y_train, x_val, y_val, input_size, no_hidden_layers, hidden_layer_size, no_of_classes, wt_initialisation, optimiser, activation_function, batch_size, eta, epoch, momentum, beta, beta1, beta2, loss_func, lambd, do_wandb_log, plot_conf_mat = x_train1, y_train1, x_val1, y_val1, input_size1, no_hidden_layers1, hidden_layer_size1, no_of_classes1, wt_initialisation1, optimiser1, activation_function1, batch_size1, eta1, epoch1, momentum1, beta1, beta11, beta21, loss_func1, lambd1, do_wandb_log1, plot_conf_mat1\n",
        "\n",
        "    nn = NN(layers, wt_initialisation)\n",
        "    if com == \"SGD\":\n",
        "      y_pred, y = nn.batch_grad_descent(x_train, y_train, x_val, y_val, no_of_classes, layers, activation_function, eta, batch_size, epoch, loss_func, lambd, do_wandb_log)\n",
        "    \n",
        "    if com == \"MOMENTUM\":\n",
        "      y_pred, y = nn.momentum_grad_descent(x_train, y_train, x_val, y_val, no_of_classes, layers, activation_function, batch_size, eta, epoch, momentum, loss_func, lambd, do_wandb_log)\n",
        "    if com == \"NAG\":\n",
        "      y_pred, y = nn.nesterov_gradient_descent(x_train, y_train, x_val, y_val, no_of_classes, layers, activation_function, batch_size, eta, epoch, momentum, loss_func, lambd, do_wandb_log)\n",
        "\n",
        "    if com == \"RMSPROP\":\n",
        "      y_pred, y = nn.rmsprop_gradient_descent(x_train, y_train, x_val, y_val, no_of_classes, layers, activation_function, batch_size, eta, epoch, beta, loss_func, lambd, do_wandb_log)\n",
        "\n",
        "    if com == \"ADAM\":\n",
        "      y_pred, y = nn.adam_gradient_descent(x_train, y_train, x_val, y_val, no_of_classes, layers, activation_function, batch_size, eta, epoch, beta1, beta2, loss_func, lambd, do_wandb_log)\n",
        "\n",
        "    if com == \"NADAM\":\n",
        "      y_pred, y = nn.nadam_gradient_descent(x_train, y_train, x_val, y_val, no_of_classes, layers, activation_function, batch_size, eta, epoch, beta1, beta2, loss_func, lambd, do_wandb_log)\n",
        "\n",
        "    if plot_conf_mat == True:\n",
        "      y_pred = np.argmax(y_pred, axis = 1)\n",
        "      print(y_pred.shape, y.shape)\n",
        "      nn.Confunsion_Matrix_Plot(y_pred, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HnFwkFiPf8h",
        "outputId": "40495408-0bec-4b7a-eff2-23bda3de9f30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MOMENTUM\n",
            "1 Iteration No :  \t Train Loss\t 1.3771880969669095\n",
            "1 Iteration No :  \t Validate Loss\t 1.3943857296706024\n",
            "1 Iteration No :  \t Train Accuracy\t 71.8\n",
            "1 Iteration No :  \t Validate Accuracy\t 71.38333333333333\n",
            "---------------------------------------------------------\n",
            "MOMENTUM\n",
            "2 Iteration No :  \t Train Loss\t 1.371456966250227\n",
            "2 Iteration No :  \t Validate Loss\t 1.3884127189063793\n",
            "2 Iteration No :  \t Train Accuracy\t 71.50185185185185\n",
            "2 Iteration No :  \t Validate Accuracy\t 71.1\n",
            "---------------------------------------------------------\n",
            "MOMENTUM\n",
            "3 Iteration No :  \t Train Loss\t 1.3691146987642662\n",
            "3 Iteration No :  \t Validate Loss\t 1.3859262867621032\n",
            "3 Iteration No :  \t Train Accuracy\t 71.52037037037037\n",
            "3 Iteration No :  \t Validate Accuracy\t 71.1\n",
            "---------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# main(x_train, y_train, x_val, y_val, input_size, no_hidden_layers, hidden_layer_size, no_of_classes, wt_initialisation, optimiser, activation_function, batch_size, eta, epoch, momentum, beta, beta1, beta2, loss_func, lambd, do_wandb_log, plot_conf_mat)\n",
        "main(x_train, y_train, x_val, y_val, 784, 1, 128, 10, \"random\", \"momentum\", \"tanh\", 32, 0.0006, 3, 0.3, beta, beta1, beta2, \"cross_entropy\", 5, False, False)\n",
        "# eps in rms, adam, nadam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbnKwBE1P0ft"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
